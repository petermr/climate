<?xml version="1.0" encoding="UTF-8"?>
<p>The reframing of the risk question from the prediction space to the decision space may seem uncomfortable from a physical science perspective, but is in fact quite orthodox from the perspective of statistical inference. Despite the widespread use of 
 <italic>p</italic>-values as an ostensibly objective measure of statistical significance, the inference derived from data concerning a particular hypothesis is far from a straightforward matter and involves many assumptions [
 <xref rid="RSPA20190013C66" ref-type="bibr">66</xref>]. In the Neyman–Pearson framework, the inference problem is regularized by placing it in a decision context between two alternative hypotheses, which takes into account the possibility of both type 1 and type 2 errors [
 <xref rid="RSPA20190013C67" ref-type="bibr">67</xref>]. In the Bayesian framework, the strength of evidence between these alternative hypotheses (
 <italic>H</italic>
 <sub>1</sub> and 
 <italic>H</italic>
 <sub>2</sub>) provided by the data 
 <italic>D</italic> is given by 
 <disp-formula id="RSPA20190013M6.1">
  <label>6.1</label>
  <math id="DM9">
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>H</mi>
       <mn>2</mn>
      </msub>
     </mrow>
     <mrow>
      <mo stretchy="false">|</mo>
     </mrow>
     <mi>D</mi>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mo stretchy="false">(</mo>
     <mrow>
      <msub>
       <mi>H</mi>
       <mn>1</mn>
      </msub>
     </mrow>
     <mrow>
      <mo stretchy="false">|</mo>
     </mrow>
     <mi>D</mi>
     <mo stretchy="false">)</mo>
    </mrow>
   </mfrac>
   <mo>=</mo>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mo stretchy="false">(</mo>
     <mi>D</mi>
     <mrow>
      <mo stretchy="false">|</mo>
     </mrow>
     <mrow>
      <msub>
       <mi>H</mi>
       <mn>2</mn>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mo stretchy="false">(</mo>
     <mi>D</mi>
     <mrow>
      <mo stretchy="false">|</mo>
     </mrow>
     <mrow>
      <msub>
       <mi>H</mi>
       <mn>1</mn>
      </msub>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mfrac>
   <mfrac>
    <mrow>
     <mi>P</mi>
     <mo stretchy="false">(</mo>
     <mrow>
      <mrow>
       <msub>
        <mi>H</mi>
        <mn>2</mn>
       </msub>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
    <mrow>
     <mi>P</mi>
     <mo stretchy="false">(</mo>
     <mrow>
      <mrow>
       <msub>
        <mi>H</mi>
        <mn>1</mn>
       </msub>
      </mrow>
     </mrow>
     <mo stretchy="false">)</mo>
    </mrow>
   </mfrac>
   <mo>,</mo>
  </math>
 </disp-formula> which follows directly from Bayes' theorem. The Bayes factor 
 <italic>P</italic>(
 <italic>D</italic>|
 <italic>H</italic>
 <sub>2</sub>)/
 <italic>P</italic>(
 <italic>D</italic>|
 <italic>H</italic>
 <sub>1</sub>) is independent of the prior likelihoods 
 <italic>P</italic>(
 <italic>H</italic>
 <sub>2</sub>) and 
 <italic>P</italic>(
 <italic>H</italic>
 <sub>1</sub>), so can be considered objective, but it does not represent any sort of absolute knowledge—only an increment in knowledge, relative to the prior beliefs.
</p>
