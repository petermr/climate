<?xml version="1.0" encoding="UTF-8"?>
<p>Each modeling algorithm was calibrated using data from a random selection of 75% of the input dataset and tested using the remaining 25% data. To avoid the possible bias from the data split, this process was repeated 10 times for each model algorithm. The relative performance of the 100 models (10 modeling algorithm × 10 replicates) for each species was evaluated using the area under the receiver operating characteristic curve (AUC; Hanley &amp; McNeil, 
 <xref rid="ece35817-bib-0018" ref-type="ref">1982</xref>) and true skill statistic (TSS; Allouche, Tsoar, &amp; Kadmon, 
 <xref rid="ece35817-bib-0002" ref-type="ref">2006</xref>). As TSS is a threshold‐dependent evaluation metric, the threshold that maximized the sum of sensitivity (true positive rate) and specificity (true negative rate) was used (Liu, White, &amp; Newell, 
 <xref rid="ece35817-bib-0037" ref-type="ref">2013</xref>). Higher AUC or TSS scores represent better model performance. Following previous studies (Lasram et al., 
 <xref rid="ece35817-bib-0027" ref-type="ref">2010</xref>; Wang, Liu, et al., 
 <xref rid="ece35817-bib-0060" ref-type="ref">2018</xref>), AUC scores between 0.7–0.8 were classified as “fair” and 0.8–1 as “good,” TSS scores between 0.4–0.6 were classified as “fair” and 0.6–1 as “good.” Only models with AUC scores higher than 0.8 and TSS scores higher than 0.45 were selected and used to predict potential distribution for given species.
</p>
