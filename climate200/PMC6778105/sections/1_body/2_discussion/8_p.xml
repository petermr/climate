<?xml version="1.0" encoding="UTF-8"?>
<p id="Par38">Second, future research should implement research designs that allow for causal inferences. Whereas all research designs are valid tools for knowledge production, establishing causality requires specific research designs. Not only RCTs can be used to achieve this, but they are the gold-standard to test causal effects. Future research should focus on establishing to what extent a particular intervention—and that intervention alone—contributes to behaviour change. This is established by a valid comparison group which has the same characteristics, on average, as the treatment group in the absence of the intervention, and remains unaffected by the intervention. Randomised assignment of participants is the most recommended procedure to achieve a valid comparison group. However, many real-world circumstances prevent this randomisation, but researchers can turn to alternative impact evaluation methods
 <sup>
  <xref ref-type="bibr" rid="CR39">39</xref>
 </sup>, such as interrupted time series, instrumental variables (i.e., variables outside the control of individuals that influence participation in the intervention but are otherwise irrelevant), regression discontinuity design, differences-in-differences or matching (for when intervention assignment rules are not clear). These more pragmatic, quasi-experimental methods may be faster, cheaper and logistically more feasible than RCTs, and could provide valuable information
 <sup>
  <xref ref-type="bibr" rid="CR40">40</xref>,
  <xref ref-type="bibr" rid="CR41">41</xref>
 </sup>. Results should, nevertheless, be presented with due caution. Simple before-and-after analyses, or descriptive comparisons of programmes with different characteristics provide the least trustworthy estimates.
</p>
