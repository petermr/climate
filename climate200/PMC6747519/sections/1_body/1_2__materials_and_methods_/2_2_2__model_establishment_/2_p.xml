<?xml version="1.0" encoding="UTF-8"?>
<p>To evaluate model performance, we randomly divided the data into training (75%) and validation data sets (25%). To consider the uncertainty introduced by training and validation set splitting, we generated 10 models using cross-validation methods with 10 iterations. The maximum, minimum and standard deviation obtained from repeated runs were used to estimate possible deviations due to arbitrary data splits, all of which were used for final prediction. Modeling results revealed the species–environment relationship and the contribution rate of each variable in the model. We used the jackknife test to investigate the importance of an individual climate variable for MaxEnt predictions and used the receiver operating characteristic curve (ROC) to assess the accuracy of the model. The area enclosed by the curve and the abscissa is the area under the curve (AUC) value, and the model performance range measured by the AUC is 0–1. The larger the value, the more the species distribution deviates from the random distribution. The evaluation criteria are as follows: AUC &gt; 0.9 is very good; 0.8 &lt; AUC &lt; 0.9 is good; 0.7 &lt; AUC &lt; 0.8 is acceptable; 0.6 &lt; AUC &lt; 0.7 is bad; AUC &lt; 0.6 is invalid [
 <xref rid="B35-ijerph-16-03185" ref-type="bibr">35</xref>]. The main advantage of this method is that it is independent of the threshold and the evaluation results are more objective [
 <xref rid="B36-ijerph-16-03185" ref-type="bibr">36</xref>].
</p>
